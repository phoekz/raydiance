<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>raydiance - blog</title>
    <style>
        @font-face {
            font-family: "Source Sans Pro - Regular";
            src: url("fonts/SourceSansPro-Regular.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Sans Pro - SemiBold";
            src: url("fonts/SourceSansPro-SemiBold.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Code Pro";
            src: url("fonts/SourceCodePro-Regular.ttf") format("truetype");
        }

        * {
            font-family: "Source Sans Pro - Regular";
        }

        h1,
        h2,
        h3 {
            font-family: "Source Sans Pro - SemiBold";
            font-weight: 400;
        }

        code {
            font-family: "Source Code Pro";
        }

        body {
            line-height: 1.5;
            font-size: 18px;
        }

        header {
            font-size: 22px;
            border-bottom: 2px solid #bbb
        }

        a {
            color: darkorange;
            text-decoration: none;
        }

        section {
            max-width: 800px;
            margin: 0 auto;
        }

        article {
            padding-bottom: 16px;
            border-bottom: 1px solid #bbb
        }

        .article-date {
            margin: 20px 0;
            color: #666;
        }

        .commit {
            font-size: 14px;
            color: #666;
        }

        footer {
            padding: 20px 10px 40px 10px;
        }
    </style>
</head>

<body>
    <section>
        <!-- Header -->
        <header>
            <h1>Raydiance Blog</h1>
        </header>

        <!-- Posts -->
        <article>
            <h2>A new shiny specular BRDF</h2>
            <div class="article-date">2023-01-31</div>

            <img src="images/20230130-192913.png">

            <h3>A very brief overview of microfacet models</h3>



            <p>
                A popular way to model physically based reflective surfaces is
                to imagine that such surfaces are built of small perfect mirrors
                called microfacets. Conceptually, each facet has a random height
                and orientation. Randomness of these properties determine the
                roughness of the surface. These microfacets are so tiny that
                they can be modeled with functions, as opposed to being modeled
                with geometry, or with normal maps for example. We implemented
                the popular
                <a href="https://inst.eecs.berkeley.edu//~cs283/sp13/lectures/cookpaper.pdf">Cook-Torrance model</a>,
                which is defined by the following functions:
            </p>

            <ul>
                <li>
                    Normal distribution function <strong>D</strong>. It
                    describes how much the microfacets varies wrt. the
                    microsurface normal. Disney model uses the popular
                    <a href="https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf">GGX distribution</a>,
                    so that is what we are going to use as well.
                </li>
                <li>
                    Masking-shadowing function <strong>G</strong>. It describes
                    the ratio of microfacets which are shadowed or masked when
                    viewed from any pair of incoming and outgoing directions.
                    There is a great deep dive by
                    <a href="https://jcgt.org/published/0003/02/03/paper.pdf">Eric Heitz</a>
                    which is all about understanding how this function works.
                    Our implementation uses the Smith's height-correlated
                    masking-shadowing function from this paper. Note that the
                    Disney model uses the separable variant, but according to
                    Heitz, the height-correlated version is more accurate.
                </li>
                <li>
                    Fresnel reflectance <strong>F</strong>, which describes how
                    much incoming light is reflected from a microfacet. Here we
                    use the same Schlick's approximation as we did with Disney
                    diffuse.
                </li>
            </ul>

            Multiplying these together results in microfacet reflectance, which
            we use for shading just like the diffuse version.

            <h3>Microfacet models in practice</h3>

            <div style="text-align: center; font-size: 0.8em;"><img src="images/20230129-140531-bug.png">What happens if we use a wrong coordinate system</div>

            <p>
                Translating math to source code has a couple of gotchas you need
                to be aware of:
            </p>

            <ul>
                <li>
                    Different papers have different naming conventions (incoming
                    vs light, outgoing vs view), different coordinate systems (z
                    is up vs y is up) which can get confusing fast if we are not
                    being really consistent.
                </li>
                <li>
                    Floating point inaccuracies can make various terms go to
                    infinity or become a <code>NaN</code>. For example, if
                    either the incident or outgoing rays are really close to
                    being perpendicular to the surface normal, the cosine of
                    their angles wrt. surface normal approaches zero. Then, any
                    expression which divides by this value results in infinity.
                    The program won't crash, but the image will slowly become
                    more and more corrupted with black or white pixels. Extra
                    care must be taken to clamp such values to a small positive
                    number to avoid dividing by zero.
                </li>
                <li>
                    Sometimes the sampled vector appears below the hemisphere.
                    In these cases we simply discard the whole sample, because
                    those samples have zero reflectance.
                </li>
            </ul>

            <p>
                We also use the trick from <code>pbrt</code> where they perform
                all BRDF calculations in a local space, where the surface normal
                is always equal to <code>(0,1,0)</code>. In this local space
                many computations simplify a lot, for example computing the dot
                product between a vector against the surface normal is simply
                the y-component of the vector. We can use the same orthonormal
                basis from the previous posts to go from world to local space,
                and once we are done with all BRDF math, we can transform the
                results back to world space.
            </p>

            <h3>Integrating microfacets with Disney diffuse BRDF</h3>

            <img src="images/20230131-010421-metallic.gif">

            <p>
                The new specular BRDF introduced three new parameters to our
                materials:
            </p>

            <ul>
                <li>Metallic: "metallic-ness". It is a linear blend between 0=dielectric and 1=metallic. The "specular color" is derived from the base color.</li>
                <li>Specular: incident specular amount. This basically replaces the explicit index of refraction. It is currently fixed to 0.5, because we don't have a way to get it from GLB yet.</li>
                <li>Anisotropic: degree of anisotropy. Controls the aspect ratio of the specular highlight. It's currently disabled, because our model does not have tangents.</li>
            </ul>

            <p>
                The Disney paper states that their model allows their artist to
                simply blend between any two combination of parameters and have
                reasonable results. In the small example we interpolate metallic
                from 0 to 1.
            </p>

            <p>
                We now have an interesting problem where we need to choose which
                BRDF to sample from. The Disney paper doesn't describe a method
                for it, so in our implementation we simply draw a new random
                variable which selects between diffuse and specular BRDF, based
                on the metallic parameter. For example, if the metallic value is
                0.5, both diffuse and specular BRDFs are equally likely to be
                chosen.
            </p>

            <h3>Animated BRDF visualizations</h3>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 5px;">
                <img src="images/20230131-000000-microfacet-reflection-r-scalar-sobol-hemisphere.png">
                <img src="images/20230131-000000-microfacet-reflection-r-incoming-sobol-hemisphere.png">
                <img src="images/20230131-000000-microfacet-reflection-r-scalar-sobol-angle.png">
                <img src="images/20230131-000000-microfacet-reflection-r-incoming-sobol-angle.png">
            </div>

            <p>
                We dramatically improved the capabilities of sample placement
                visualizer from the previous post. The visualizations are now
                animated, can render different text for each frame, and
                reflectance is now visualized separately from probability
                density functions.
            </p>

            <p>
                The animations on the left have a fixed incoming direction,
                while the roughness is being interpolated between 0 and 1. The
                animations on the right have a fixed 0.25 roughness while the
                incoming direction (visualized as a purple square) is being
                swung along the x-axis.
            </p>

            <p>
                The animations are encoded in <a href="https://en.wikipedia.org/wiki/APNG">APNG</a> format.
                We chose APNG because:
            </p>

            <ul>
                <li>GIF is too low quality due to limited 256-color palette limitation.</li>
                <li>WebP's crate takes very long to build, and has slightly worse support than APNG.</li>
                <li>Traditional video formats are not as convenient for short looping animations.</li>
            </ul>

            <p>
                These crates were used to create the animations:
            </p>

            <ul>
                <li><a href="https://crates.io/crates/apng"><code>apng</code></a> for encoding APNG's from <a href="https://crates.io/crates/image"><code>image</code></a> buffers.</li>
                <li><a href="https://crates.io/crates/png"><code>png</code></a> so we can properly call into <a href="https://crates.io/crates/apng"><code>apng</code></a>.</li>
                <li><a href="https://crates.io/crates/easer"><code>easer</code></a> for <a href="https://easings.net/">easing functions</a>. We use <code>easeInOutCubic</code> for interesting movement.</li>
                <li><a href="https://crates.io/crates/imageproc"><code>imageproc</code></a> for drawing text on <a href="https://crates.io/crates/image"><code>image</code></a> buffers.</li>
                <li><a href="https://crates.io/crates/rusttype"><code>rusttype</code></a> for loading TTF fonts for <a href="https://crates.io/crates/imageproc"><code>imageproc</code></a>.</li>
                <li><a href="https://crates.io/crates/rayon"><code>rayon</code></a> for simple data-parallelism to speed up animation renders.</li>
            </ul>

            <h3>A simple interactive material editor</h3>

            <video width="800" height="450" autoplay loop muted playsinline>
                <source src="images/20230131-014036.mp4" type="video/mp4">
            </video>

            <p>
                Having to recompile the program or exporting a new scene from
                Blender every time we needed to change the roughness or metallic
                value quickly became a major bottleneck. Since our raytracing is
                already progressive, we can easily implement simple material
                edits and have the raytracer re-render the image at each change.
            </p>

            <p>
                We will rewrite this utility in the future once we do a bigger
                user interface overhaul.
            </p>

            <h3>Visualizing normals</h3>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 5px;">
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230131-012835.png" width="100%">Raytraced</div>
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230131-012842.png" width="100%">Rasterized</div>
            </div>

            <p>
                While we were hunting for bugs in our specular BRDFs, we added a
                simple way to visualize shading normals both in raytraced and
                rasterized scenes. We will add visualizations for texture
                coordinates and tangents in the future.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/bf578f68f0c0906a9cb50548eb3e830cd69222d6">
                    <code>bf578f68</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Visualizing sample placement</h2>
            <div class="article-date">2023-01-20</div>

            <img src="images/20230120-011300-top.png" width="100%">
            <img src="images/20230120-011300-side.png" width="100%">

            <p>
                Importance sampling Disney specular models is more challenging
                than our current diffuse models. To improve our chances, we
                created a small tool which visualizes where the sample are
                placed around the hemisphere.
            </p>

            <p>
                To make sure the tool works, we used our existing uniform and
                cosine-weighted hemisphere samplers. We also added two
                additional sample sequences for comparison:
            </p>

            <ol>
                <li><code>grid</code> sequence, which uniformly samples the unit square.</li>
                <li><code>sobol</code> sequence, which is provided by <a href="https://crates.io/crates/sobol_burley"><code>sobol_burley</code></a> crate. The crate implementation is based on <a href="https://www.jcgt.org/published/0009/04/01/">Practical Hash-based Owen Scrambling</a>.</li>
            </ol>

            <p>
                The <code>top</code> plots view the hemisphere from above in
                cartesian space. The <code>side</code> plots in
                <code>x=[0,2pi]</code> and <code>y=[0,pi/2]</code> hemispherical
                space.

                For both sets of plots, the background brightness correspond to
                the magnitude of <code>cos(theta)</code>.
            </p>

            <p>
                Looking at the plots, we can intuitively say that <code>cosine</code>
                performs better than <code>uniform</code>
                sampling, because it places samples closer to the bright spots.
                Similarly, <code>sobol</code> performs better than
                <code>random</code> and <code>grid</code>.
            </p>

            <p>
                Note that the new sequences are not currently available for
                rendering. We will revisit low-discrepancy sequences later.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/f5b806749234a4086d84388d7264c0f2fd43122a">
                    <code>f5b80674</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Implementing Disney BRDF - Diffuse model</h2>
            <div class="article-date">2023-01-19</div>

            <img src="images/20230119-035800.gif">

            <p>
                <a href="https://blog.selfshadow.com/publications/s2012-shading-course/burley/s2012_pbs_disney_brdf_notes_v3.pdf">Disney principled BRDF</a>

                is a popular physically based reflectance model developed by
                Brent Burley et al. at Disney. It is adopted by

                <a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/shader/principled.html">Blender</a>,
                <a href="https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf">Unreal Engine</a>,
                <a href="https://seblagarde.files.wordpress.com/2015/07/course_notes_moving_frostbite_to_pbr_v32.pdf">Frostbite</a>,

                and many other productions. The team at Disney analyzed the

                <a href="https://www.merl.com/brdf/">MERL BRDF Database</a>

                and fit their model based on MERL's empirical measurements.
                Their goal was to create an artist-friendly model with as few
                parameters as possible. These are the design principles from the
                course notes:
            </p>

            <ol>
                <li>Intuitive rather than physical parameters should be used.</li>
                <li>There should be as few parameters as possible.</li>
                <li>Parameters should be zero to one over their plausible range.</li>
                <li>Parameters should be allowed to be pushed beyond their plausible range where it makes sense.</li>
                <li>All combinations of parameters should be as robust and plausible as possible</li>
            </ol>

            <p>
                The full Disney model is a combination of multiple scattering
                models, some of which we are currently not very experienced
                with. To avoid getting overwhelmed, we are going to study and
                implement one model at the time, starting with the diffuse
                model.
            </p>

            <p>
                Disney's diffuse model is a novel empirical model which attempts
                to solve the over darkening that comes from the Lambertian
                diffuse model. This darkening happens at grazing angles, i.e.
                the angle between incoming light and outgoing light is close to
                0. Disney models this by adding a
                <a href="https://en.wikipedia.org/wiki/Fresnel_equations">Fresnel factor</a>
                , which is approximated with
                <a href="https://en.wikipedia.org/wiki/Schlick%27s_approximation">Schlick's approximation</a>.
                Basically, the grazing angle responses become brighter the
                rougher the surface becomes.
            </p>

            <p>
                The difference in the comparison above can be a bit subtle. The
                main difference is that the edges on the cube are slightly
                brighter compared to Lambert model, and the right side of the
                cube, which also appears brighter.
            </p>

            <p>
                Currently, our implementation intentionally ignores the "sheen"
                term and the subsurface scattering approximation. We will come
                back to them later. Also, any roughness value below 1.0 looks
                incorrect, because our current implementation has no specular
                terms.
            </p>

            <p>References:</p>
            <ul>
                <li><a href="https://blog.selfshadow.com/publications/s2012-shading-course/burley/s2012_pbs_disney_brdf_notes_v3.pdf">SIGGRAPH 2012 - Physically Based Shading at Disney - Course Notes</a></li>
                <li><a href="https://schuttejoe.github.io/post/disneybsdf/">Joe Schutte - Rendering the Moana Island Scene Part 1: Implementing the Disney BSDF</a></li>
                <li><a href="http://shihchinw.github.io/2015/07/implementing-disney-principled-brdf-in-arnold.html">Shih-Chin - Implementing Disney Principled BRDF in Arnold</a></li>
                <li><a href="https://github.com/mmp/pbrt-v3/blob/master/src/materials/disney.cpp"><code>pbrt-v3 - disney.cpp</code></a></li>
                <li><a href="https://github.com/wdas/brdf">Disney BRDF Explorer</a></li>
            </ul>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/c43f282e24a6eecb54fa3361a6e5e192453d0d8d">
                    <code>c43f282e</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Basic texture support</h2>
            <div class="article-date">2023-01-15</div>

            <img src="images/20230115-204600.webp">

            <p>
                Raydiance now supports texture mapped surfaces. We used multiple
                shortcuts to get a basic implementation going:
            </p>

            <ul>
                <li>Only nearest neighbor filtering is supported.</li>
                <li>No mipmaps.</li>
                <li>No anisotropic filtering.</li>
                <li>Only <code>R8G8B8A8_UNORM</code> pixel format is supported.</li>
            </ul>

            <p>
                We will revisit these shortcuts later, once our scenes get more
                complicated.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/41a05a78d6fbba82436faece9815c4e8b3da9951">
                    <code>41a05a78</code>
                </a>
            </div>
        </article>

        <article>
            <h2>New user interface</h2>
            <div class="article-date">2023-01-15</div>

            <img src="images/20230115-140255.webp">

            <p>
                It was time to replace the window title hacks and random
                keybindings with a real graphical user interface. We use the the
                excellent
                <a href="https://github.com/ocornut/imgui">Dear ImGui</a>
                library. Since our project is written in Rust, we use
                <a href="https://crates.io/crates/imgui"><code>imgui</code></a> and
                <a href="https://crates.io/crates/imgui"><code>imgui-winit-support</code></a>
                crates to wrap the original C++ library and interface with
                <a href="https://crates.io/crates/winit"><code>winit</code></a>.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/9fb5a3800a337b5d663e1c83932e03fe96abfe0f">
                    <code>9fb5a380</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Cosine-weighted hemisphere sampling</h2>
            <div class="article-date">2023-01-13</div>

            <img src="images/20230113-134900.webp">

            <p>
                To get a cleaner picture, we could increase the number of
                samples, but that would increase render times, which forces us
                to find ways to make the renderer run faster. Alternatively, we
                could be smarter at using our limited number of samples. This
                way of reducing noise in Monte Carlo simulations is called
                <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>.

                For our simple diffuse cube scene, one of the most impactful
                techniques is cosine-weighted hemisphere sampling. Since the
                rendering equation has a cosine term, it makes sense to sample
                from a distribution that is similar to that. Our implementation
                is based on
                <a href="https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Cosine-WeightedHemisphereSampling"><code>pbrt</code></a>.
            </p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 5px;">
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230113-134400-uniform.gif" width="100%">Uniform sampling</div>
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230113-134400-cosine.gif" width="100%">Cosine-weighted sampling</div>
            </div>

            <p>
                Here is the comparison between uniform sampling. It is clear that
                with identical sample counts, cosine-weighted sampling results
                in much cleaner picture than uniform sampling. And it does it at
                pretty much equivalent time.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/2e0e31997c20714ccc6a3735cf3a5c0a899f9ab9">
                    <code>2e0e3199</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Progressive rendering</h2>
            <div class="article-date">2023-01-12</div>

            <img src="images/20230112-215200.webp">

            <p>
                Previously we waited until the entire image was completed before
                displaying to the screen. In this commit we redesigned the path
                tracing loop to render progressively and submit intermediate
                frames as soon as they are finished. This significantly improves
                interactivity.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/f50c3b6f92eedd52e43406ee4960d3b50b5025ac">
                    <code>f50c3b6f</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Interactive CPU path tracer</h2>
            <div class="article-date">2023-01-12</div>

            <img src="images/20230112-152800.webp" title="Rendered with 16 spp">

            <p>
                This commit merges the CPU path tracer with the Vulkan renderer,
                and makes the camera interactive. As soon as the path tracer
                finishes rendering, the image is uploaded to the GPU and
                rendered on the window. We can also toggle between raytraced
                image and rasterized image to confirm that both renderers are in
                sync.
            </p>

            <p>
                To keep the Vulkan renderer running while the CPU is busy path
                tracing, we need to run the path tracer on its own thread. To
                communicate across threads boundaries, we use Rust standard
                library
                <a href="https://doc.rust-lang.org/std/sync/mpsc/index.html"><code>std::sync::mpsc:channel</code></a>.
                The main thread sends camera transforms to the path tracer, and
                the path tracer sends the rendered images back to the main
                thread. The path tracer thread blocks on the channel in prevent
                busy looping.
            </p>

            <p>
                For displaying path traced images on the window, we set up a
                both the uploader and the rendering pipeline for the image.
                Uploading image data is not very exciting, refer to this page
                from
                <a href="https://vulkan-tutorial.com/Texture_mapping/Images">Vulkan Tutorial</a>.

                However, for rendering we used two tricks:
            </p>

            <ol>
                <li>
                    To render a fullscreen textured quad, you don't actually
                    need to create vertex buffers, set up vertex inputs, and so
                    on. With this <a href="https://www.saschawillems.de/blog/2016/08/13/vulkan-tutorial-on-rendering-a-fullscreen-quad-without-buffers/">trick</a>,
                    you can use <code>gl_VertexIndex</code> intrinsic in the
                    vertex shader to build a huge triangle and then calculate
                    the UVs within it. This saves a lot of boilerplate.
                </li>
                <li>
                    In Vulkan if you want to sample a texture in your fragment
                    shader, you need to create descriptor pools, descriptor set
                    layouts, allocate descriptor sets, make sure pipeline
                    layouts are correct, bind the descriptor sets, and so on. With
                    <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VK_KHR_push_descriptor"><code>VK_KHR_push_descriptor</code></a>
                    extension, it is possible to simplify this process
                    significantly. Enabling it allows you to push the descriptor
                    right before issuing the draw call, saving a lot of
                    boilerplate. We still have to create one descriptor set
                    layout for the pipeline layout object, but that is not too
                    bad compared to what we had to do before, just to bind one
                    texture to a shader.
                </li>
            </ol>

            <p>
                As a side, <code>vulkan.rs</code> is reaching 2000 LOC, which is
                getting pretty challenging to work with. We will have to break
                it down soon.
            </p>

            <p>
                The path tracing performance is not great because we are still
                using only one thread. It is also why the image is noisier than
                the previous post, since we had to lower the sample count to get
                barely interactive frame rates. We will address the noise and
                the performance in upcoming commits.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/956e4bf6a4fdb2db5ea790acac1227c0297e58ac">
                    <code>956e4bf6</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Path tracing on CPU</h2>
            <div class="article-date">2023-01-11</div>

            <img src="images/20230111-231544.png" title="Rendered with 64 spp">

            <p>
                Finally, we are getting into the main feature of
                <code>raydiance</code>: rendering pretty images using ray
                tracing. We start with a pure CPU implementation. The plan is to
                develop and maintain the CPU version as the reference
                implementation for the future GPU version, mainly because it is
                much easier to work with compared to debugging shaders. The
                Vulkan renderer we've built so far serves as the visual
                interface for
                <code>raydiance</code>, and later, we will use Vulkan's ray
                tracing extensions to build the GPU version.
            </p>

            <p>
                Our implementation use the following components:
            </p>

            <ol>
                <li>Ray vs triangle intersection: <a href="https://jcgt.org/published/0002/01/05/">Watertight Ray/Triangle Intersection</a></li>
                <li>Orthonormal basis: <a href="https://jcgt.org/published/0006/01/01/">Building an Orthonormal Basis, Revisited</a></li>
                <li>Uniformly distributed random numbers: <a href="https://crates.io/crates/rand"><code>rand</code></a> and <a href="https://crates.io/crates/rand_pcg"><code>rand_pcg</code></a> crates</li>
                <li>Uniform hemisphere sampling: <a href="https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#UniformlySamplingaHemisphere"><code>pbrt</code></a></li>
                <li>Acceleration structure (bounding volume hierarchy): <a href="https://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Bounding_Volume_Hierarchies"><code>pbrt</code></a></li>
            </ol>

            <p>
                We put this together into a path tracing loop, where we bounce
                rays until they hit the sky or they have bounced too many times.
                Each pixel in the image does this a number of times, averages
                all the samples and writes out the final color to the image
            </p>

            <p>
                For materials, we start with the simplest one: Lambertian
                material, which scatters incoming light equally in all
                directions. However, there is a subtle detail in Lambertian
                BRDF, which is that you have to divide the base color with π.
                Here's the explanation from <a href="https://www.pbr-book.org/3ed-2018/Reflection_Models/Lambertian_Reflection"><code>pbrt</code></a>.
            </p>

            <p>
                For lights, we assume that every ray that bounces off the scene
                will hit “the sky”. In that case, we just return some bright
                white color.
            </p>

            <p>
                For anti-aliasing, we randomly shift the subpixel position of
                each primary ray and apply the box filter over the samples. With
                enough samples, this naturally resolves into a nice image with
                no aliasing. <a href="https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Image_Reconstruction"><code>pbrt</code></a>'s
                image reconstruction chapter has better alternatives for box
                filter, which we might look into later.
            </p>

            <p>
                For performance, we currently run the path tracer in a single
                CPU thread. Obviously this is not ideal, but for such a tiny
                image and low sample count, the rendering only takes a couple of
                seconds. We will come back to this once we need to make the path
                tracer run at interactive speeds.
            </p>

            <p>
                Currently raydiance doesn't display the path traced image
                anywhere, for this post we wrote the image out to the disk. We
                will fix this soon.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/4ade2d5b2acc3da8fabb5d275b9152171ed01ea9">
                    <code>4ade2d5b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Adding multisampled anti-aliasing (MSAA)</h2>
            <div class="article-date">2023-01-10</div>

            <img src="images/20230110-001539.png" width="100%">

            <p>
                This was pretty easy. Similarly to depth buffer, we create a new
                color buffer which will be multisampled. The depth buffer is
                also updated to support multisampling. Then we update all the
                <code>resolve*</code> fields in <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VkRenderingAttachmentInfo.html"><code>VkRenderingAttachmentInfo</code></a>,
                and finally we add multisampling state to our pipeline. No more
                jagged edges.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ca2a23caaa5e9d9b321a50389af492fbc708b560">
                    <code>ca2a23ca</code>
                </a>
            </div>
        </article>

        <article>
            <h2>More triangles, cameras, light, and depth</h2>
            <div class="article-date">2023-01-09</div>

            <img src="images/20230109-181800.webp">

            <p>
                A lot has happened since our single hardcoded triangle. We can
                now render shaded, depth tested, transformed, indexed triangle
                lists, with perspective projection.
            </p>

            <h3>Loading and rendering GLTF scenes</h3>

            <img src="images/20230109-182433.png" width="100%">

            <p>
                We created a simple "cube on a plane" scene in Blender. Each
                object has a "Principled BSDF" material attached to it. This
                material is <a
                    href="https://docs.blender.org/manual/en/latest/addons/import_export/scene_gltf2.html#extensions">well
                    supported</a> by Blender's GLTF exporter, which is what we will
                use for our application. GLTF supports text formats, but we will
                export the scene in binary (<code>.glb</code>) for efficiency.
            </p>

            <p>
                To load the <code>.glb</code> file, we use <a href="https://crates.io/crates/gltf"><code>gltf</code></a>
                crate. Immediately after loading, we pick out the interesting
                fields (cameras, meshes, materials) and convert them into our
                <a
                    href="https://github.com/phoekz/raydiance/blob/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2/src/assets.rs#L3-L35">internal
                    data format</a>. This internal format is designed to be easy to
                upload to the GPU. We also do aggressive validation in order to
                catch any properties that we don't support yet, such as
                textures, meshes that do not have normals, and so on. Our
                internal formats represent matrices and vectors with types from
                <a href="https://crates.io/crates/nalgebra"><code>nalgebra</code></a>
                crate. To turn our internal formats into byte slices <a
                    href="https://crates.io/crates/bytemuck"><code>bytemuck</code></a> crate.
            </p>

            <p>
                Before we can render, we need to upload geometry data to the
                GPU. For now, we assume the number of meshes is much less than
                4096 (on most Windows hosts the <a
                    href="https://vulkan.gpuinfo.org/displaydevicelimit.php?platform=windows&name=maxMemoryAllocationCount"><code>maxMemoryAllocationCount</code></a>
                is 4096). This allows us to cheat and allocate buffers for each
                mesh. The better way to handle allocations is make a few large
                allocations and sub-allocate within those, which we can do
                ourselves, or use a library like <a
                    href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator"><code>VulkanMemoryAllocator</code></a>.
                We will come back to memory allocators in the future.
            </p>

            <p>
                To render, we will have to work out the perspective projection,
                the view transform and object transforms from GLTF. We also add
                rotation transform to animate the cube. We pre-multiply all
                transforms and upload the final matrix to the vertex shader
                using <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#descriptorsets-push-constants">push
                    constants</a>. The base color of the mesh is also packed into a
                push constant. Push constants are great for small data, because
                we can avoid:
            </p>

            <ol>
                <li>Descriptor set layouts, descriptor pools, descriptor sets</li>
                <li>Uniform buffers, which would have to be double buffered to avoid stalls</li>
                <li>Synchronizing updates to uniform buffers</li>
            </ol>

            <p>
                As a side, while looking into push constants, we learned about
                <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VK_KHR_push_descriptor"><code>VK_KHR_push_descriptor</code></a>.
                This extension sounds like it could further simplify working
                with Vulkan, which is really exciting. We will come back to it
                in the future once we get into texture mapping.
            </p>

            <h3>Depth testing with <code>VK_KHR_dynamic_rendering</code></h3>

            <img src="images/20230109-190941.png">

            <p>
                Depth testing requires a depth texture, which we create at
                startup, and re-create when the window changes size. To enable depth testing with
                <code>VK_KHR_dynamic_rendering</code>, we had to extend our
                graphics pipeline with a new structure called
                <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VkPipelineRenderingCreateInfo"><code>VkPipelineRenderingCreateInfo</code></a>,
                and also add color blend state which was previously left out.
                One additional pipeline barrier was required to transition the
                depth texture for rendering.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2">
                    <code>cb1bcc19</code>
                </a>
            </div>

        </article>

        <article>
            <h2>The first triangle</h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-193100.webp">

            <p>
                This is the simplest triangle example rendered without any
                device memory allocations. The triangle is hard coded in the
                vertex shader and we index into its attributes with vertex
                index.
            </p>

            <p>
                We added a simple shader compiling step in
                <a href="https://doc.rust-lang.org/cargo/reference/build-scripts.html"><code>build.rs</code></a>
                which builds
                <code>.glsl</code> into <code>.spv</code> using Google's <a
                    href="https://github.com/google/shaderc/tree/main/glslc"><code>glslc</code></a>,
                which is included in <a href="https://vulkan.lunarg.com/sdk/home">LunarG's Vulkan
                    SDK</a>.
            </p>

            <p>
                Next we will implement device memory allocations in order to
                load a 3D model from a file.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/c8f9ef2c0f3b51ddfd71f33ec086fca1531051ab">
                    <code>c8f9ef2c</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Clearing window with <code>VK_KHR_dynamic_rendering</code></h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-170100.webp">

            <p>
                After around 1000 LOC, we have a barebones Vulkan application which:
            </p>

            <ol>
                <li>Load Vulkan with <a href="https://crates.io/crates/ash"><code>ash</code></a> crate.</li>
                <li>Creates Vulkan instance with <code>VK_LAYER_KHRONOS_validation</code> and debug utilities.</li>
                <li>Creates window surface with <a
                        href="https://crates.io/crates/ash-window"><code>ash-window</code></a>
                    and
                    <a href="https://crates.io/crates/raw-window-handle"><code>raw-window-handle</code></a> crates.
                </li>
                <li>Creates logical device and queues.</li>
                <li>Creates command pool and buffers.</li>
                <li>Creates swapchain.</li>
                <li>Creates semaphores and fences for host to host and host to device synchronization.</li>
                <li>Clears the screen with a different color every frame.</li>
            </ol>

            <p>
                We also handle tricky situations such as user resizing the window and minimizing the window.
            </p>

            <p>
                Notably we are not creating render passes or framebuffers, thanks to
                <code>VK_KHR_dynamic_rendering</code>. We do have to specify some
                render pass parameters when we record command buffers, but reducing
                the number of API abstractions simplifies the implementation
                signifcantly. We used this <a
                    href="https://github.com/SaschaWillems/Vulkan/blob/313ac10de4a765997ddf5202c599e4a0ca32c8ca/examples/dynamicrendering/dynamicrendering.cpp">example</a>
                as a reference.
            </p>

            <p>
                Everything is written under <code>main()</code> with minimal
                abstractions and with liberal use of <code>unsafe</code>. We
                will do a <a href="https://caseymuratori.com/blog_0015">semantic
                    compression</a>
                pass later once we have learned more about how the program should be
                laid out.
            </p>

            <p>
                Next we will continue with more Vulkan code to get a triangle on the screen.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/0f6d7f1bf1b22d1fff43e87080c854eadb3e459d">
                    <code>0f6d7f1b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Hello, <code>winit</code>!</h2>
            <div class="article-date">2023-01-07</div>

            <img src="images/20230107-161828.png" />

            <p>
                Before anything interesting can happen, we are going to need a window to draw on. We use <a
                    href="https://crates.io/crates/winit"><code>winit</code></a>
                crate for windowing and handling inputs. For convenience, we
                bound the Escape key to close the window and center the window
                in the middle of the primary monitor.
            </p>

            <p>
                For simple logging we use <a href="https://crates.io/crates/log"><code>log</code></a> and <a
                    href="https://crates.io/crates/env_logger"><code>env_logger</code></a>,
                and for application-level error handling we use <a
                    href="https://crates.io/crates/anyhow"><code>anyhow</code></a>.
            </p>

            <p>
                Next we are going to slog through a huge amount of Vulkan
                boilerplate to begin drawing something on our blank window.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ff4c31c2c6c2039d33bfd07865448da963febfd6">
                    <code>ff4c31c2</code>
                </a>
            </div>
        </article>

        <!-- Footer -->
        <footer>
            <div style="float: left"><a href="https://github.com/phoekz/raydiance">GitHub</a></div>
            <div style="text-align: right; float: right">© 2023 Vinh Truong</div>
        </footer>
    </section>
</body>

</html>