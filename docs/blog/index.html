<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>raydiance - blog</title>
    <style>
        @font-face {
            font-family: "Source Sans Pro - Regular";
            src: url("fonts/SourceSansPro-Regular.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Sans Pro - SemiBold";
            src: url("fonts/SourceSansPro-SemiBold.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Code Pro";
            src: url("fonts/SourceCodePro-Regular.ttf") format("truetype");
        }

        * {
            font-family: "Source Sans Pro - Regular";
        }

        h1,
        h2,
        h3 {
            font-family: "Source Sans Pro - SemiBold";
            font-weight: 400;
        }

        code {
            font-family: "Source Code Pro";
        }

        body {
            line-height: 1.5;
            font-size: 18px;
        }

        header {
            font-size: 22px;
            border-bottom: 2px solid #bbb
        }

        a {
            color: darkorange;
            text-decoration: none;
        }

        section {
            max-width: 800px;
            margin: 0 auto;
        }

        article {
            padding-bottom: 16px;
            border-bottom: 1px solid #bbb
        }

        .article-date {
            margin: 20px 0;
            color: #666;
        }

        .commit {
            font-size: 14px;
            color: #666;
        }

        footer {
            padding: 20px 10px 40px 10px;
        }
    </style>
</head>

<body>
    <section>
        <!-- Header -->
        <header>
            <h1>Raydiance Blog</h1>
        </header>

        <!-- Posts -->
        <article>
            <h2>Cosine-weighted hemisphere sampling</h2>
            <div class="article-date">2023-01-13</div>

            <img src="images/20230113-134900.webp">

            <p>
                To get a cleaner picture, we could increase the number of
                samples, but that would increase render times, which forces us
                to find ways to make the renderer run faster. Alternatively, we
                could be smarter at using our limited number of samples. This
                way of reducing noise in Monte Carlo simulations is called
                <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>.

                For our simple diffuse cube scene, one of the most impactful
                techniques is cosine-weighted hemisphere sampling. Since the
                rendering equation has a cosine term, it makes sense to sample
                from a distribution that is similar to that. Our implementation
                is based on
                <a href="https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#Cosine-WeightedHemisphereSampling"><code>pbrt</code></a>.
            </p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 5px;">
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230113-134400-uniform.gif" width="100%">Uniform sampling</div>
                <div style="text-align: center; font-size: 0.8em;"><img src="images/20230113-134400-cosine.gif" width="100%">Cosine-weighted sampling</div>
            </div>

            <p>
                Here is the comparison between uniform sampling. It is clear that
                with identical sample counts, cosine-weighted sampling results
                in much cleaner picture than uniform sampling. And it does it at
                pretty much equivalent time.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/2e0e31997c20714ccc6a3735cf3a5c0a899f9ab9">
                    <code>2e0e3199</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Progressive rendering</h2>
            <div class="article-date">2023-01-12</div>

            <img src="images/20230112-215200.webp">

            <p>
                Previously we waited until the entire image was completed before
                displaying to the screen. In this commit we redesigned the path
                tracing loop to render progressively and submit intermediate
                frames as soon as they are finished. This significantly improves
                interactivity.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/f50c3b6f92eedd52e43406ee4960d3b50b5025ac">
                    <code>f50c3b6f</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Interactive CPU path tracer</h2>
            <div class="article-date">2023-01-12</div>

            <img src="images/20230112-152800.webp" title="Rendered with 16 spp">

            <p>
                This commit merges the CPU path tracer with the Vulkan renderer,
                and makes the camera interactive. As soon as the path tracer
                finishes rendering, the image is uploaded to the GPU and
                rendered on the window. We can also toggle between raytraced
                image and rasterized image to confirm that both renderers are in
                sync.
            </p>

            <p>
                To keep the Vulkan renderer running while the CPU is busy path
                tracing, we need to run the path tracer on its own thread. To
                communicate across threads boundaries, we use Rust standard
                library
                <a href="https://doc.rust-lang.org/std/sync/mpsc/index.html"><code>std::sync::mpsc:channel</code></a>.
                The main thread sends camera transforms to the path tracer, and
                the path tracer sends the rendered images back to the main
                thread. The path tracer thread blocks on the channel in prevent
                busy looping.
            </p>

            <p>
                For displaying path traced images on the window, we set up a
                both the uploader and the rendering pipeline for the image.
                Uploading image data is not very exciting, refer to this page
                from
                <a href="https://vulkan-tutorial.com/Texture_mapping/Images">Vulkan Tutorial</a>.

                However, for rendering we used two tricks:
            </p>

            <ol>
                <li>
                    To render a fullscreen textured quad, you don't actually
                    need to create vertex buffers, set up vertex inputs, and so
                    on. With this <a href="https://www.saschawillems.de/blog/2016/08/13/vulkan-tutorial-on-rendering-a-fullscreen-quad-without-buffers/">trick</a>,
                    you can use <code>gl_VertexIndex</code> intrinsic in the
                    vertex shader to build a huge triangle and then calculate
                    the UVs within it. This saves a lot of boilerplate.
                </li>
                <li>
                    In Vulkan if you want to sample a texture in your fragment
                    shader, you need to create descriptor pools, descriptor set
                    layouts, allocate descriptor sets, make sure pipeline
                    layouts are correct, bind the descriptor sets, and so on. With
                    <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VK_KHR_push_descriptor"><code>VK_KHR_push_descriptor</code></a>
                    extension, it is possible to simplify this process
                    significantly. Enabling it allows you to push the descriptor
                    right before issuing the draw call, saving a lot of
                    boilerplate. We still have to create one descriptor set
                    layout for the pipeline layout object, but that is not too
                    bad compared to what we had to do before, just to bind one
                    texture to a shader.
                </li>
            </ol>

            <p>
                As a side, <code>vulkan.rs</code> is reaching 2000 LOC, which is
                getting pretty challenging to work with. We will have to break
                it down soon.
            </p>

            <p>
                The path tracing performance is not great because we are still
                using only one thread. It is also why the image is noisier than
                the previous post, since we had to lower the sample count to get
                barely interactive frame rates. We will address the noise and
                the performance in upcoming commits.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/956e4bf6a4fdb2db5ea790acac1227c0297e58ac">
                    <code>956e4bf6</code>
                </a>
            </div>

        </article>

        <article>
            <h2>Path tracing on CPU</h2>
            <div class="article-date">2023-01-11</div>

            <img src="images/20230111-231544.png" title="Rendered with 64 spp">

            <p>
                Finally, we are getting into the main feature of
                <code>raydiance</code>: rendering pretty images using ray
                tracing. We start with a pure CPU implementation. The plan is to
                develop and maintain the CPU version as the reference
                implementation for the future GPU version, mainly because it is
                much easier to work with compared to debugging shaders. The
                Vulkan renderer we've built so far serves as the visual
                interface for
                <code>raydiance</code>, and later, we will use Vulkan's ray
                tracing extensions to build the GPU version.
            </p>

            <p>
                Our implementation use the following components:
            </p>

            <ol>
                <li>Ray vs triangle intersection: <a href="https://jcgt.org/published/0002/01/05/">Watertight Ray/Triangle Intersection</a></li>
                <li>Orthonormal basis: <a href="https://jcgt.org/published/0006/01/01/">Building an Orthonormal Basis, Revisited</a></li>
                <li>Uniformly distributed random numbers: <a href="https://crates.io/crates/rand"><code>rand</code></a> and <a href="https://crates.io/crates/rand_pcg"><code>rand_pcg</code></a> crates</li>
                <li>Uniform hemisphere sampling: <a href="https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#UniformlySamplingaHemisphere"><code>pbrt</code></a></li>
                <li>Acceleration structure (bounding volume hierarchy): <a href="https://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Bounding_Volume_Hierarchies"><code>pbrt</code></a></li>
            </ol>

            <p>
                We put this together into a path tracing loop, where we bounce
                rays until they hit the sky or they have bounced too many times.
                Each pixel in the image does this a number of times, averages
                all the samples and writes out the final color to the image
            </p>

            <p>
                For materials, we start with the simplest one: Lambertian
                material, which scatters incoming light equally in all
                directions. However, there is a subtle detail in Lambertian
                BRDF, which is that you have to divide the base color with π.
                Here's the explanation from <a href="https://www.pbr-book.org/3ed-2018/Reflection_Models/Lambertian_Reflection"><code>pbrt</code></a>.
            </p>

            <p>
                For lights, we assume that every ray that bounces off the scene
                will hit “the sky”. In that case, we just return some bright
                white color.
            </p>

            <p>
                For anti-aliasing, we randomly shift the subpixel position of
                each primary ray and apply the box filter over the samples. With
                enough samples, this naturally resolves into a nice image with
                no aliasing. <a href="https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Image_Reconstruction"><code>pbrt</code></a>'s
                image reconstruction chapter has better alternatives for box
                filter, which we might look into later.
            </p>

            <p>
                For performance, we currently run the path tracer in a single
                CPU thread. Obviously this is not ideal, but for such a tiny
                image and low sample count, the rendering only takes a couple of
                seconds. We will come back to this once we need to make the path
                tracer run at interactive speeds.
            </p>

            <p>
                Currently raydiance doesn't display the path traced image
                anywhere, for this post we wrote the image out to the disk. We
                will fix this soon.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/4ade2d5b2acc3da8fabb5d275b9152171ed01ea9">
                    <code>4ade2d5b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Adding multisampled anti-aliasing (MSAA)</h2>
            <div class="article-date">2023-01-10</div>

            <img src="images/20230110-001539.png" width="100%">

            <p>
                This was pretty easy. Similarly to depth buffer, we create a new
                color buffer which will be multisampled. The depth buffer is
                also updated to support multisampling. Then we update all the
                <code>resolve*</code> fields in <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VkRenderingAttachmentInfo.html"><code>VkRenderingAttachmentInfo</code></a>,
                and finally we add multisampling state to our pipeline. No more
                jagged edges.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ca2a23caaa5e9d9b321a50389af492fbc708b560">
                    <code>ca2a23ca</code>
                </a>
            </div>
        </article>

        <article>
            <h2>More triangles, cameras, light, and depth</h2>
            <div class="article-date">2023-01-09</div>

            <img src="images/20230109-181800.webp">

            <p>
                A lot has happened since our single hardcoded triangle. We can
                now render shaded, depth tested, transformed, indexed triangle
                lists, with perspective projection.
            </p>

            <h3>Loading and rendering GLTF scenes</h3>

            <img src="images/20230109-182433.png" width="100%">

            <p>
                We created a simple "cube on a plane" scene in Blender. Each
                object has a "Principled BSDF" material attached to it. This
                material is <a
                    href="https://docs.blender.org/manual/en/latest/addons/import_export/scene_gltf2.html#extensions">well
                    supported</a> by Blender's GLTF exporter, which is what we will
                use for our application. GLTF supports text formats, but we will
                export the scene in binary (<code>.glb</code>) for efficiency.
            </p>

            <p>
                To load the <code>.glb</code> file, we use <a href="https://crates.io/crates/gltf"><code>gltf</code></a>
                crate. Immediately after loading, we pick out the interesting
                fields (cameras, meshes, materials) and convert them into our
                <a
                    href="https://github.com/phoekz/raydiance/blob/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2/src/assets.rs#L3-L35">internal
                    data format</a>. This internal format is designed to be easy to
                upload to the GPU. We also do aggressive validation in order to
                catch any properties that we don't support yet, such as
                textures, meshes that do not have normals, and so on. Our
                internal formats represent matrices and vectors with types from
                <a href="https://crates.io/crates/nalgebra"><code>nalgebra</code></a>
                crate. To turn our internal formats into byte slices <a
                    href="https://crates.io/crates/bytemuck"><code>bytemuck</code></a> crate.
            </p>

            <p>
                Before we can render, we need to upload geometry data to the
                GPU. For now, we assume the number of meshes is much less than
                4096 (on most Windows hosts the <a
                    href="https://vulkan.gpuinfo.org/displaydevicelimit.php?platform=windows&name=maxMemoryAllocationCount"><code>maxMemoryAllocationCount</code></a>
                is 4096). This allows us to cheat and allocate buffers for each
                mesh. The better way to handle allocations is make a few large
                allocations and sub-allocate within those, which we can do
                ourselves, or use a library like <a
                    href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator"><code>VulkanMemoryAllocator</code></a>.
                We will come back to memory allocators in the future.
            </p>

            <p>
                To render, we will have to work out the perspective projection,
                the view transform and object transforms from GLTF. We also add
                rotation transform to animate the cube. We pre-multiply all
                transforms and upload the final matrix to the vertex shader
                using <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#descriptorsets-push-constants">push
                    constants</a>. The base color of the mesh is also packed into a
                push constant. Push constants are great for small data, because
                we can avoid:
            </p>

            <ol>
                <li>Descriptor set layouts, descriptor pools, descriptor sets</li>
                <li>Uniform buffers, which would have to be double buffered to avoid stalls</li>
                <li>Synchronizing updates to uniform buffers</li>
            </ol>

            <p>
                As a side, while looking into push constants, we learned about
                <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VK_KHR_push_descriptor"><code>VK_KHR_push_descriptor</code></a>.
                This extension sounds like it could further simplify working
                with Vulkan, which is really exciting. We will come back to it
                in the future once we get into texture mapping.
            </p>

            <h3>Depth testing with <code>VK_KHR_dynamic_rendering</code></h3>

            <img src="images/20230109-190941.png">

            <p>
                Depth testing requires a depth texture, which we create at
                startup, and re-create when the window changes size. To enable depth testing with
                <code>VK_KHR_dynamic_rendering</code>, we had to extend our
                graphics pipeline with a new structure called
                <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VkPipelineRenderingCreateInfo"><code>VkPipelineRenderingCreateInfo</code></a>,
                and also add color blend state which was previously left out.
                One additional pipeline barrier was required to transition the
                depth texture for rendering.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2">
                    <code>cb1bcc19</code>
                </a>
            </div>

        </article>

        <article>
            <h2>The first triangle</h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-193100.webp">

            <p>
                This is the simplest triangle example rendered without any
                device memory allocations. The triangle is hard coded in the
                vertex shader and we index into its attributes with vertex
                index.
            </p>

            <p>
                We added a simple shader compiling step in
                <a href="https://doc.rust-lang.org/cargo/reference/build-scripts.html"><code>build.rs</code></a>
                which builds
                <code>.glsl</code> into <code>.spv</code> using Google's <a
                    href="https://github.com/google/shaderc/tree/main/glslc"><code>glslc</code></a>,
                which is included in <a href="https://vulkan.lunarg.com/sdk/home">LunarG's Vulkan
                    SDK</a>.
            </p>

            <p>
                Next we will implement device memory allocations in order to
                load a 3D model from a file.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/c8f9ef2c0f3b51ddfd71f33ec086fca1531051ab">
                    <code>c8f9ef2c</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Clearing window with <code>VK_KHR_dynamic_rendering</code></h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-170100.webp">

            <p>
                After around 1000 LOC, we have a barebones Vulkan application which:
            </p>

            <ol>
                <li>Load Vulkan with <a href="https://crates.io/crates/ash"><code>ash</code></a> crate.</li>
                <li>Creates Vulkan instance with <code>VK_LAYER_KHRONOS_validation</code> and debug utilities.</li>
                <li>Creates window surface with <a
                        href="https://crates.io/crates/ash-window"><code>ash-window</code></a>
                    and
                    <a href="https://crates.io/crates/raw-window-handle"><code>raw-window-handle</code></a> crates.
                </li>
                <li>Creates logical device and queues.</li>
                <li>Creates command pool and buffers.</li>
                <li>Creates swapchain.</li>
                <li>Creates semaphores and fences for host to host and host to device synchronization.</li>
                <li>Clears the screen with a different color every frame.</li>
            </ol>

            <p>
                We also handle tricky situations such as user resizing the window and minimizing the window.
            </p>

            <p>
                Notably we are not creating render passes or framebuffers, thanks to
                <code>VK_KHR_dynamic_rendering</code>. We do have to specify some
                render pass parameters when we record command buffers, but reducing
                the number of API abstractions simplifies the implementation
                signifcantly. We used this <a
                    href="https://github.com/SaschaWillems/Vulkan/blob/313ac10de4a765997ddf5202c599e4a0ca32c8ca/examples/dynamicrendering/dynamicrendering.cpp">example</a>
                as a reference.
            </p>

            <p>
                Everything is written under <code>main()</code> with minimal
                abstractions and with liberal use of <code>unsafe</code>. We
                will do a <a href="https://caseymuratori.com/blog_0015">semantic
                    compression</a>
                pass later once we have learned more about how the program should be
                laid out.
            </p>

            <p>
                Next we will continue with more Vulkan code to get a triangle on the screen.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/0f6d7f1bf1b22d1fff43e87080c854eadb3e459d">
                    <code>0f6d7f1b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Hello, <code>winit</code>!</h2>
            <div class="article-date">2023-01-07</div>

            <img src="images/20230107-161828.png" />

            <p>
                Before anything interesting can happen, we are going to need a window to draw on. We use <a
                    href="https://crates.io/crates/winit"><code>winit</code></a>
                crate for windowing and handling inputs. For convenience, we
                bound the Escape key to close the window and center the window
                in the middle of the primary monitor.
            </p>

            <p>
                For simple logging we use <a href="https://crates.io/crates/log"><code>log</code></a> and <a
                    href="https://crates.io/crates/env_logger"><code>env_logger</code></a>,
                and for application-level error handling we use <a
                    href="https://crates.io/crates/anyhow"><code>anyhow</code></a>.
            </p>

            <p>
                Next we are going to slog through a huge amount of Vulkan
                boilerplate to begin drawing something on our blank window.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ff4c31c2c6c2039d33bfd07865448da963febfd6">
                    <code>ff4c31c2</code>
                </a>
            </div>
        </article>

        <!-- Footer -->
        <footer>
            <div style="float: left"><a href="https://github.com/phoekz/raydiance">GitHub</a></div>
            <div style="text-align: right; float: right">© 2023 Vinh Truong</div>
        </footer>
    </section>
</body>

</html>