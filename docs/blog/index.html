<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>raydiance - blog</title>
    <style>
        @font-face {
            font-family: "Source Sans Pro - Regular";
            src: url("fonts/SourceSansPro-Regular.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Sans Pro - SemiBold";
            src: url("fonts/SourceSansPro-SemiBold.ttf") format("truetype");
        }

        @font-face {
            font-family: "Source Code Pro";
            src: url("fonts/SourceCodePro-Regular.ttf") format("truetype");
        }

        * {
            font-family: "Source Sans Pro - Regular";
        }

        h1,
        h2,
        h3 {
            font-family: "Source Sans Pro - SemiBold";
            font-weight: 400;
        }

        code {
            font-family: "Source Code Pro";
        }

        body {
            line-height: 1.5;
            font-size: 18px;
        }

        header {
            font-size: 22px;
            border-bottom: 2px solid #bbb
        }

        a {
            color: darkorange;
            text-decoration: none;
        }

        section {
            max-width: 800px;
            margin: 0 auto;
        }

        article {
            padding-bottom: 16px;
            border-bottom: 1px solid #bbb
        }

        .article-date {
            margin: 20px 0;
            color: #666;
        }

        .commit {
            font-size: 14px;
            color: #666;
        }

        footer {
            padding: 20px 10px 40px 10px;
        }
    </style>
</head>

<body>
    <section>
        <!-- Header -->
        <header>
            <h1>Raydiance Blog</h1>
        </header>

        <!-- Posts -->
        <article>
            <h2>Path tracing on CPU</h2>
            <div class="article-date">2023-01-11</div>

            <img src="images/20230111-231544.png" title="Rendered with 64 spp">

            <p>
                Finally, we are getting into the main feature of
                <code>raydiance</code>: rendering pretty images using ray
                tracing. We start with a pure CPU implementation. The plan is to
                develop and maintain the CPU version as the reference
                implementation for the future GPU version, mainly because it is
                much easier to work with compared to debugging shaders. The
                Vulkan renderer we've built so far serves as the visual
                interface for
                <code>raydiance</code>, and later, we will use Vulkan's ray
                tracing extensions to build the GPU version.
            </p>

            <p>
                Our implementation use the following components:
            </p>

            <ol>
                <li>Ray vs triangle intersection: <a href="https://jcgt.org/published/0002/01/05/">Watertight Ray/Triangle Intersection</a></li>
                <li>Orthonormal basis: <a href="https://jcgt.org/published/0006/01/01/">Building an Orthonormal Basis, Revisited</a></li>
                <li>Uniformly distributed random numbers: <a href="https://crates.io/crates/rand"><code>rand</code></a> and <a href="https://crates.io/crates/rand_pcg"><code>rand_pcg</code></a> crates</li>
                <li>Uniform hemisphere sampling: <a href="https://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#UniformlySamplingaHemisphere"><code>pbrt</code></a></li>
                <li>Acceleration structure (bounding volume hierarchy): <a href="https://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Bounding_Volume_Hierarchies"><code>pbrt</code></a></li>
            </ol>

            <p>
                We put this together into a path tracing loop, where we bounce
                rays until they hit the sky or they have bounced too many times.
                Each pixel in the image does this a number of times, averages
                all the samples and writes out the final color to the image
            </p>

            <p>
                For materials, we start with the simplest one: Lambertian
                material, which scatters incoming light equally in all
                directions. However, there is a subtle detail in Lambertian
                BRDF, which is that you have to divide the base color with π.
                Here's the explanation from <a href="https://www.pbr-book.org/3ed-2018/Reflection_Models/Lambertian_Reflection"><code>pbrt</code></a>.
            </p>

            <p>
                For lights, we assume that every ray that bounces off the scene
                will hit “the sky”. In that case, we just return some bright
                white color.
            </p>

            <p>
                For anti-aliasing, we randomly shift the subpixel position of
                each primary ray and apply the box filter over the samples. With
                enough samples, this naturally resolves into a nice image with
                no aliasing. <a href="https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Image_Reconstruction"><code>pbrt</code></a>'s
                image reconstruction chapter has better alternatives for box
                filter, which we might look into later.
            </p>

            <p>
                For performance, we currently run the path tracer in a single
                CPU thread. Obviously this is not ideal, but for such a tiny
                image and low sample count, the rendering only takes a couple of
                seconds. We will come back to this once we need to make the path
                tracer run at interactive speeds.
            </p>

            <p>
                Currently raydiance doesn't display the path traced image
                anywhere, for this post we wrote the image out to the disk. We
                will fix this soon.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/4ade2d5b2acc3da8fabb5d275b9152171ed01ea9">
                    <code>4ade2d5b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Adding multisampled anti-aliasing (MSAA)</h2>
            <div class="article-date">2023-01-10</div>

            <img src="images/20230110-001539.png" width="100%">

            <p>
                This was pretty easy. Similarly to depth buffer, we create a new
                color buffer which will be multisampled. The depth buffer is
                also updated to support multisampling. Then we update all the
                <code>resolve*</code> fields in <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VkRenderingAttachmentInfo.html"><code>VkRenderingAttachmentInfo</code></a>,
                and finally we add multisampling state to our pipeline. No more
                jagged edges.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ca2a23caaa5e9d9b321a50389af492fbc708b560">
                    <code>ca2a23ca</code>
                </a>
            </div>
        </article>

        <article>
            <h2>More triangles, cameras, light, and depth</h2>
            <div class="article-date">2023-01-09</div>

            <img src="images/20230109-181800.webp">

            <p>
                A lot has happened since our single hardcoded triangle. We can
                now render shaded, depth tested, transformed, indexed triangle
                lists, with perspective projection.
            </p>

            <h3>Loading and rendering GLTF scenes</h3>

            <img src="images/20230109-182433.png" width="100%">

            <p>
                We created a simple "cube on a plane" scene in Blender. Each
                object has a "Principled BSDF" material attached to it. This
                material is <a
                    href="https://docs.blender.org/manual/en/latest/addons/import_export/scene_gltf2.html#extensions">well
                    supported</a> by Blender's GLTF exporter, which is what we will
                use for our application. GLTF supports text formats, but we will
                export the scene in binary (<code>.glb</code>) for efficiency.
            </p>

            <p>
                To load the <code>.glb</code> file, we use <a href="https://crates.io/crates/gltf"><code>gltf</code></a>
                crate. Immediately after loading, we pick out the interesting
                fields (cameras, meshes, materials) and convert them into our
                <a
                    href="https://github.com/phoekz/raydiance/blob/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2/src/assets.rs#L3-L35">internal
                    data format</a>. This internal format is designed to be easy to
                upload to the GPU. We also do aggressive validation in order to
                catch any properties that we don't support yet, such as
                textures, meshes that do not have normals, and so on. Our
                internal formats represent matrices and vectors with types from
                <a href="https://crates.io/crates/nalgebra"><code>nalgebra</code></a>
                crate. To turn our internal formats into byte slices <a
                    href="https://crates.io/crates/bytemuck"><code>bytemuck</code></a> crate.
            </p>

            <p>
                Before we can render, we need to upload geometry data to the
                GPU. For now, we assume the number of meshes is much less than
                4096 (on most Windows hosts the <a
                    href="https://vulkan.gpuinfo.org/displaydevicelimit.php?platform=windows&name=maxMemoryAllocationCount"><code>maxMemoryAllocationCount</code></a>
                is 4096). This allows us to cheat and allocate buffers for each
                mesh. The better way to handle allocations is make a few large
                allocations and sub-allocate within those, which we can do
                ourselves, or use a library like <a
                    href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator"><code>VulkanMemoryAllocator</code></a>.
                We will come back to memory allocators in the future.
            </p>

            <p>
                To render, we will have to work out the perspective projection,
                the view transform and object transforms from GLTF. We also add
                rotation transform to animate the cube. We pre-multiply all
                transforms and upload the final matrix to the vertex shader
                using <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#descriptorsets-push-constants">push
                    constants</a>. The base color of the mesh is also packed into a
                push constant. Push constants are great for small data, because
                we can avoid:
            </p>

            <ol>
                <li>Descriptor set layouts, descriptor pools, descriptor sets</li>
                <li>Uniform buffers, which would have to be double buffered to avoid stalls</li>
                <li>Synchronizing updates to uniform buffers</li>
            </ol>

            <p>
                As a side, while looking into push constants, we learned about
                <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VK_KHR_push_descriptor"><code>VK_KHR_push_descriptor</code></a>.
                This extension sounds like it could further simplify working
                with Vulkan, which is really exciting. We will come back to it
                in the future once we get into texture mapping.
            </p>

            <h3>Depth testing with <code>VK_KHR_dynamic_rendering</code></h3>

            <img src="images/20230109-190941.png">

            <p>
                Depth testing requires a depth texture, which we create at
                startup, and re-create when the window changes size. To enable depth testing with
                <code>VK_KHR_dynamic_rendering</code>, we had to extend our
                graphics pipeline with a new structure called
                <a
                    href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#VkPipelineRenderingCreateInfo"><code>VkPipelineRenderingCreateInfo</code></a>,
                and also add color blend state which was previously left out.
                One additional pipeline barrier was required to transition the
                depth texture for rendering.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/cb1bcc1975e3860b7208cffb4286fec3e91cc5d2">
                    <code>cb1bcc19</code>
                </a>
            </div>

        </article>

        <article>
            <h2>The first triangle</h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-193100.webp">

            <p>
                This is the simplest triangle example rendered without any
                device memory allocations. The triangle is hard coded in the
                vertex shader and we index into its attributes with vertex
                index.
            </p>

            <p>
                We added a simple shader compiling step in
                <a href="https://doc.rust-lang.org/cargo/reference/build-scripts.html"><code>build.rs</code></a>
                which builds
                <code>.glsl</code> into <code>.spv</code> using Google's <a
                    href="https://github.com/google/shaderc/tree/main/glslc"><code>glslc</code></a>,
                which is included in <a href="https://vulkan.lunarg.com/sdk/home">LunarG's Vulkan
                    SDK</a>.
            </p>

            <p>
                Next we will implement device memory allocations in order to
                load a 3D model from a file.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/c8f9ef2c0f3b51ddfd71f33ec086fca1531051ab">
                    <code>c8f9ef2c</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Clearing window with <code>VK_KHR_dynamic_rendering</code></h2>
            <div class="article-date">2023-01-08</div>

            <img src="images/20230108-170100.webp">

            <p>
                After around 1000 LOC, we have a barebones Vulkan application which:
            </p>

            <ol>
                <li>Load Vulkan with <a href="https://crates.io/crates/ash"><code>ash</code></a> crate.</li>
                <li>Creates Vulkan instance with <code>VK_LAYER_KHRONOS_validation</code> and debug utilities.</li>
                <li>Creates window surface with <a
                        href="https://crates.io/crates/ash-window"><code>ash-window</code></a>
                    and
                    <a href="https://crates.io/crates/raw-window-handle"><code>raw-window-handle</code></a> crates.
                </li>
                <li>Creates logical device and queues.</li>
                <li>Creates command pool and buffers.</li>
                <li>Creates swapchain.</li>
                <li>Creates semaphores and fences for host to host and host to device synchronization.</li>
                <li>Clears the screen with a different color every frame.</li>
            </ol>

            <p>
                We also handle tricky situations such as user resizing the window and minimizing the window.
            </p>

            <p>
                Notably we are not creating render passes or framebuffers, thanks to
                <code>VK_KHR_dynamic_rendering</code>. We do have to specify some
                render pass parameters when we record command buffers, but reducing
                the number of API abstractions simplifies the implementation
                signifcantly. We used this <a
                    href="https://github.com/SaschaWillems/Vulkan/blob/313ac10de4a765997ddf5202c599e4a0ca32c8ca/examples/dynamicrendering/dynamicrendering.cpp">example</a>
                as a reference.
            </p>

            <p>
                Everything is written under <code>main()</code> with minimal
                abstractions and with liberal use of <code>unsafe</code>. We
                will do a <a href="https://caseymuratori.com/blog_0015">semantic
                    compression</a>
                pass later once we have learned more about how the program should be
                laid out.
            </p>

            <p>
                Next we will continue with more Vulkan code to get a triangle on the screen.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/0f6d7f1bf1b22d1fff43e87080c854eadb3e459d">
                    <code>0f6d7f1b</code>
                </a>
            </div>
        </article>

        <article>
            <h2>Hello, <code>winit</code>!</h2>
            <div class="article-date">2023-01-07</div>

            <img src="images/20230107-161828.png" />

            <p>
                Before anything interesting can happen, we are going to need a window to draw on. We use <a
                    href="https://crates.io/crates/winit"><code>winit</code></a>
                crate for windowing and handling inputs. For convenience, we
                bound the Escape key to close the window and center the window
                in the middle of the primary monitor.
            </p>

            <p>
                For simple logging we use <a href="https://crates.io/crates/log"><code>log</code></a> and <a
                    href="https://crates.io/crates/env_logger"><code>env_logger</code></a>,
                and for application-level error handling we use <a
                    href="https://crates.io/crates/anyhow"><code>anyhow</code></a>.
            </p>

            <p>
                Next we are going to slog through a huge amount of Vulkan
                boilerplate to begin drawing something on our blank window.
            </p>

            <div class="commit">
                Commit:
                <a href="https://github.com/phoekz/raydiance/commit/ff4c31c2c6c2039d33bfd07865448da963febfd6">
                    <code>ff4c31c2</code>
                </a>
            </div>
        </article>

        <!-- Footer -->
        <footer>
            <div style="float: left"><a href="https://github.com/phoekz/raydiance">GitHub</a></div>
            <div style="text-align: right; float: right">© 2023 Vinh Truong</div>
        </footer>
    </section>
</body>

</html>